{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Lyrics for Sentiment Analysis (Binary Genre Classification) of Songs, Jake Schaeffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reads the CSV of Song Titles, Artists, and Lyrics\n",
    "data = pd.read_csv('songdata.csv')\n",
    "# removes '\\n's that were scattered throughout the lyrics text\n",
    "data['text'].replace('\\n','',regex=True,inplace=True)\n",
    "\n",
    "# reads CSV of top songs from \n",
    "aa_data = pd.read_csv('top_songs.csv')\n",
    "\n",
    "# reads Jan Wiebe's Subjectivity Lexicon\n",
    "lexicon = pd.read_csv(\"subjectivity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_title = data['song'].values\n",
    "song_artist = data['artist'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylast\n",
    "\n",
    "# You have to have your own unique two values for API_KEY and API_SECRET\n",
    "API_KEY = \"my key\"\n",
    "API_SECRET = \"my secret\"\n",
    "\n",
    "network = pylast.LastFMNetwork(api_key=API_KEY, api_secret=API_SECRET,\n",
    "                               username=username, password_hash=password_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collects the last.fm key for each song\n",
    "lasted_tracks = []\n",
    "for i in range(len(song_title)):\n",
    "    track = network.get_track(song_artist[i],song_title[i])\n",
    "    lasted_tracks.append(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collects all of the tags for each song in the dataset.\n",
    "# This block took about 10 hours to run so I saved the results to a CSV\n",
    "# which was loaded in below to be used when I came back to this project.\n",
    "top_list = []\n",
    "for i in range(len(lasted_tracks)):\n",
    "    try:\n",
    "        top_tag = lasted_tracks[i].get_top_tags()\n",
    "        top_list.append(top_tag)\n",
    "    except:\n",
    "        top_list.append(\"NA\")\n",
    "    if i%10 == 0:\n",
    "        print \"at stage\",i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CSV of collected tag data\n",
    "tags_formatted = pd.read_csv(\"songs_tags\")\n",
    "listed = pd.DataFrame(lasted_tracks)\n",
    "tags_formatted['names'] = listed[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The API calls returned a long string that contained the tags buried\n",
    "# amongst messy text. This parses the strings and extracts the tag.\n",
    "def substring_after(s, delim):\n",
    "    return s.partition(delim)[2]\n",
    "\n",
    "topdf = tags_formatted.drop(tags_formatted.columns[[0]],1)\n",
    "complex_tags = topdf.iloc[:,0][1]\n",
    "sub = substring_after(complex_tags[2],\"(u'\")\n",
    "\n",
    "complex_tags = complex_tags.split(\"Tag\")\n",
    "sub = substring_after(complex_tags[4],\"(u'\")\n",
    "tag = sub.split(\"'\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This was code that does the same thing as the above cell. The tags\n",
    "# stored as a variable, and loaded in from my saved CSV required different\n",
    "# processing steps (very annoyingly).\n",
    "tags = []\n",
    "weights = []\n",
    "\n",
    "A = []\n",
    "\n",
    "for i in range(len(topdf.iloc[:,0])):\n",
    "    my_list = []\n",
    "    complex_tags = str(topdf.iloc[:,0][i])\n",
    "    complex_tags = complex_tags.split(\"Tag\")\n",
    "    for j in range(len(complex_tags)):\n",
    "        sub = substring_after(complex_tags[j],\"(u'\")\n",
    "        tag = sub.split(\"'\")[0]\n",
    "        my_list.append(tag)\n",
    "    A.append(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adf = pd.DataFrame(A)\n",
    "Adf = pd.read_csv('songs_tags')\n",
    "Adf=Adf.drop(Adf.columns[[0]], axis=1)\n",
    "# Adf['title'] = song_title\n",
    "Adf['artist'] = song_artist\n",
    "\n",
    "count = 0\n",
    "no_tags = []\n",
    "Gets index of all songs that had no tags\n",
    "for i in range(len(Adf.iloc[:,0].values)):\n",
    "    if Adf.iloc[:,0].values[i] == None:\n",
    "        no_tags.append(i)\n",
    "Removes songs from Dataframe with no tags, shrinking DataFrame from\n",
    "57,650 entries to 46,010\n",
    "Adf.drop(Adf.index[no_tags],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block goes through Jan Wiebe's semantic dictionary, notes\n",
    "whether a tag is a word that matches positive or negative sentiment in \n",
    "her dictionary, and classifies songs based on whether there are more\n",
    "positive or negative tags.\n",
    "\n",
    "Again, this script took about 30 minutes to run (my mistake of\n",
    "iterating through a Pandas df using .iloc) so the results were also\n",
    "saved to a CSV to be referenced later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "word_list = lexicon.iloc[:,0].values\n",
    "word_list = word_list.tolist()\n",
    "\n",
    "classifier = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(len(Adf)):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for j in range(len(Adf.iloc[i])):\n",
    "        single_tag = Adf.iloc[i][j]\n",
    "        try:\n",
    "            ind = word_list.index(single_tag.lower())\n",
    "        except:\n",
    "            continue\n",
    "        if lexicon.iloc[:,1][ind] == \"positive\":\n",
    "            pos_count += 1\n",
    "        if lexicon.iloc[:,1][ind] == \"negative\":\n",
    "            neg_count += 1\n",
    "    if pos_count > neg_count:\n",
    "        classifier.append(1)\n",
    "    elif neg_count > pos_count:\n",
    "        classifier.append(-1)\n",
    "    else:\n",
    "        classifier.append(0)\n",
    "    if i%1000 == 0:\n",
    "        print \"stage\", i\n",
    "\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "print \"Script took\",runtime, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block creates unique song/artist string combos as many songs have\n",
    "# the same exact title and were messing with the merge.\n",
    "Adf['sentiment'] = classifier\n",
    "Training_S = Adf[['title','artist','sentiment']]\n",
    "data.rename(columns={'song': 'title', 'text': 'lyrics'}, inplace=True)\n",
    "Training_S.to_csv('title_sentiment')\n",
    "data.to_csv('cleaned_data')\n",
    "Training_S['merger'] = Adf['title'] + Adf['artist']\n",
    "data['merger'] = data['title'] + data['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "This step equalizes the datasets so that there are an even number of\n",
    "positive and negative classified songs.\n",
    "Merged = Training_S.merge(data, on='merger', how='left').drop_duplicates()\n",
    "Merged = Merged[['title_x','artist_x','lyrics','sentiment']]\n",
    "Merged.rename(columns={'title_x': 'title', 'artist_x': 'artist'}, inplace=True)\n",
    "\n",
    "PosSent = Merged[Merged.sentiment == 1]\n",
    "NegSent = Merged[Merged.sentiment == -1]\n",
    "PosSentMatch = PosSent.sample(n=len(NegSent))\n",
    "SentMatched = pd.concat([PosSentMatch, NegSent], ignore_index=True)\n",
    "SentMatched.to_csv('training_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Down In The Park</td>\n",
       "      <td>Foo Fighters</td>\n",
       "      <td>Down in the park where the machmen meet  The m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If You Love Me Baby</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>If you leave me, baby  I don't know what I'll ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chair In The Sky</td>\n",
       "      <td>Joni Mitchell</td>\n",
       "      <td>The rain slammed hard as bars  It caught me--b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Bone Movies</td>\n",
       "      <td>Ozzy Osbourne</td>\n",
       "      <td>Silver screen such a disgrace  I couldn't look...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Common Mortal Man</td>\n",
       "      <td>Free</td>\n",
       "      <td>I was on my way to a needle factory  Up and co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title         artist  \\\n",
       "0     Down In The Park   Foo Fighters   \n",
       "1  If You Love Me Baby    The Beatles   \n",
       "2   A Chair In The Sky  Joni Mitchell   \n",
       "3       No Bone Movies  Ozzy Osbourne   \n",
       "4    Common Mortal Man           Free   \n",
       "\n",
       "                                              lyrics  sentiment  \n",
       "0  Down in the park where the machmen meet  The m...          1  \n",
       "1  If you leave me, baby  I don't know what I'll ...          1  \n",
       "2  The rain slammed hard as bars  It caught me--b...          1  \n",
       "3  Silver screen such a disgrace  I couldn't look...          1  \n",
       "4  I was on my way to a needle factory  Up and co...          1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads in the cleaned dataset all of the steps above created\n",
    "SentMatched = pd.read_csv('training_data')\n",
    "SentMatched = SentMatched.drop(SentMatched.columns[[0]], axis=1)\n",
    "SentMatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SentMatched.target = SentMatched.sentiment\n",
    "lyric_data = SentMatched.lyrics\n",
    "target_data = SentMatched.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now that I have a cleaned, classified dataset, I can begin the model selection process.\n",
    "\n",
    "    The below cell builds a dictionary of features and transforms documents to feature vectors through text preprocessing, tokenizing and filtering of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5776, 26690)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(lyric_data, target_data, \n",
    "                                                    test_size=0.25, random_state=42)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "y_train.target_names = ['positive','negative']\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23403"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer supports counts of N-grams of words or consecutive\n",
    "# characters. Once fitted, the vectorizer has built a dictionary of \n",
    "# feature indices with the index value of a word in the vocabulary\n",
    "# linked to its frequency in the whole training corpus.\n",
    "count_vect.vocabulary_.get('terror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The occurrence count of words within lyrics provided a solid starting point in the classification process, but there was an issue: longer songs  have higher average count values than shorter songs, even when they might talk about the same topics.\n",
    "\n",
    "In the name of preventing this issue, I can divide the number of occurrences of each word in a lyric by the total number of words in the lyric: these new features are called tf for Term Frequencies.\n",
    "\n",
    "To provide one more refinement, I can downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is referred to in sci-kit learn as tf–idf for “Term Frequency times Inverse Document Frequency”. tf and tf_idf are calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5776, 26690)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5776, 26690)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that features have been extracted from the lyric corpus, I wanted to train a classifier to try to predict positive or negative sentiment of a song. To start, I used a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier, but the one most appropriate for word counts is the multinomial Naive Bayes classifier:\n",
    "\n",
    "To calculate P(sentiment|lyrics), the algorithm calculates P(lyrics|sentiment)*P(sentiment)/P(lyrics). P(lyrics) would have no effect on the comparisons under consideration since there was no need to compare across multiple lyrics. Thus, P(lyrics|sentiment)*P(sentiment) became the only calculation. The lyrics are represented by the series of n words that constitutes the lyrics, and the algorithm assumes that the probability of seeing any given word depends exclusively on the classified sentiment, it does not take into consideration other words in the lyrics.\n",
    "\n",
    "To return to the original calculation goal, P(sentiment|lyrics) is simply equal to the product over the set P(word(i)|sentiment) for all values from i=1 to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classifier trained, I can now test out what sentiment the model predicts for different lyric sets. +1 classification represents positive sentiment, -1 represents negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lyrics = ['happy is happy', 'sad is sad']\n",
    "X_new_counts = count_vect.transform(sample_lyrics)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a pipeline that allows the model to be trained with just one command. I'm then able to look at the accuracy of this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.7476635514 %\n"
     ]
    }
   ],
   "source": [
    "# vectorized lyrics model\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_train, y_train)  \n",
    "docs_test = X_test\n",
    "NB_predicted = text_clf.predict(docs_test)\n",
    "accuracy = np.mean(NB_predicted == y_test)\n",
    "print \"Naive Bayes Accuracy:\", accuracy*100, \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper that I'm replicating, their use of the Naive Bayes approach resulted in an average basic accuracy of 56%. While I would like to assume that it is my exceptional machine learning model parameter setting skills that resulted in this improvement, it is more likely that I was able to achieve better accuracy as a result of my order of magnitude larger data set. While the paper had a total of 420 classified songs, my dataset was comprised of 7,702 classified songs. Despite this slight improvement of the accuracy from the paper, I wanted to test another model, the linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy: 58.6708203531 %\n"
     ]
    }
   ],
   "source": [
    "# model with linear support vector machine\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "SVM_predicted = text_clf.predict(docs_test)\n",
    "accuracy = np.mean(SVM_predicted == y_test)\n",
    "print \"Linear SVM Accuracy:\", accuracy*100, \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, the linear SVM performed worse on my dataset, resulting in an accuracy of 58.7%. The paper I'm replicating did not report on a linear SVM model in their research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model Results:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "positive sentiment       0.66      0.45      0.54       969\n",
      "negative sentiment       0.58      0.77      0.66       957\n",
      "\n",
      "       avg / total       0.62      0.61      0.60      1926\n",
      "\n",
      " \n",
      "Linear SVM Model Results:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "positive sentiment       0.58      0.66      0.62       969\n",
      "negative sentiment       0.60      0.51      0.55       957\n",
      "\n",
      "       avg / total       0.59      0.59      0.58      1926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print \"Naive Bayes Model Results:\"\n",
    "print(metrics.classification_report(y_test, NB_predicted,\n",
    "    target_names=['positive sentiment','negative sentiment']))\n",
    "print \" \"\n",
    "print \"Linear SVM Model Results:\"\n",
    "print(metrics.classification_report(y_test, SVM_predicted,\n",
    "    target_names=['positive sentiment','negative sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at the metrics provided by sklearn, we can look at precision as \"how useful the search results are\", and recall as \"how complete our results are\". The two metrics are calculated independently of each other. In looking at the extrema of the results, interestingly, Negative Sentiment had recall of nearly 0.8 in the Naive Bayes model. Also, the Naive Bayes model performed slightly better than the Linear SVM model which I found surprising given Linear SVM's general regard as one of the best text classifiers. I also found it interesting that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nature of Music and Lyrical Classification:\n",
    "    \n",
    "In my dataset, there were over 3x as many songs classified as positive than negative. This is unsurprising as the database I accessed had the lyrics of popular songs, more of which will be happy than negative. In looking at my results though, I'm happy with the classification accuracy I was able to achieve. Lyrics are a much more difficult classifier than something like reviews for a few reasons.\n",
    "\n",
    "1. Songs can contain a series of negative lyrics but end on an uplifting, positive note, or vice versa. This is notable in the areas of love songs where lyricists may speak on how happy they were in a relationship, but end on a sad note because of a break-up.\n",
    "\n",
    "2. Songs may not contain any words from a typical sentiment lexicon, but still express positive or negative emotions. For example, the song ”Ocean Front Property” by George Strait includes the following stanza:\n",
    "\n",
    "I got some ocean front property in Arizona\n",
    "From my front porch you can see the sea\n",
    "I got some ocean front property in Arizona\n",
    "If you'll buy that, I'll throw the golden gate in free\n",
    "\n",
    "It's tough to find an individual word that screams \"positive\" from this selection, but taken as a whole the section seems positive. But on deeper analysis, there is no such thing as ocean front property in the landlocked state of Arizona. This is a semi-spiteful message, and its complexity would likely be entirely lost on a classifier.\n",
    "\n",
    "3. Hip-hop songs in particular suffer from the issue of containing lyrics that express positive emotions about negative events like shootings and robbery. This just contributes to the complexities a classification system is expected to pick up on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these considerations in mind, I'm satisfied with the results my algorithm was able to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Identifying the Emotional Polarity of Song Lyrics through Natural\n",
    "Language Processing (Oudenne and Chasins)\n",
    "https://people.eecs.berkeley.edu/~schasins/papers/identifyingEmotionalPolarity.pdf\n",
    "\n",
    "2. 55000+ Song Lyrics from LyricsFreak \n",
    "https://www.kaggle.com/mousehead/songlyrics\n",
    "\n",
    "3. Jan Wiebe's Subjectivity Lexicon\n",
    "http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
